{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da0febdb",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using Huggingface Transformers (Boston Airbnb Reviews)\n",
    "This notebook demonstrates how to perform **sentiment analysis** on text data using the **Hugging Face `transformers`** library, specifically the `pipeline(\"sentiment-analysis\")`.  \n",
    "We'll apply it to the Boston airbnb review data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4836f",
   "metadata": {},
   "source": [
    "## What is Sentiment Analysis?\n",
    "**Sentiment analysis** (a type of text classification) detects the **polarity** of opinions expressed in text—commonly **positive**, **negative**, or **neutral**.  \n",
    "Typical uses include:\n",
    "- Monitoring customer feedback (e.g., app store reviews, support tickets)\n",
    "- Brand & reputation tracking on social media\n",
    "- Voice of customer analytics for product improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc63bd9",
   "metadata": {},
   "source": [
    "\n",
    "## Huggingface & Transformers\n",
    "- In the world of AI, [Hugging Face](https://huggingface.co) is quite the star. It’s an AI community and platform that provides state-of-the-art tools and models for Natural Language Processing (NLP). It hosts thousands of pre‑trained models and datasets for NLP, computer vision, audio and LLMs.\n",
    "- Hugging Face’s most popular offering is the **Transformers** library. The Transformers library comes packed with APIs and tools that let you easily grab and train top-notch pre-trained models.\n",
    "- For quick experimentation, the **`pipeline`** abstraction wraps tokenization, model loading, and inference with one line of code.\n",
    "\n",
    "In this notebook we will use Bert based model to conduct sentiment analysis.\n",
    "- `pipeline(\"sentiment-analysis\")` → loads a default sentiment model (e.g., a BERT or DistilBERT variant) and returns **label** and **score**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9408a3-c5bf-4a9a-aab1-ec4195803163",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BERT-Based Models in Sentiment Analysis\n",
    "\n",
    "A **BERT-based model in sentiment analysis** means using the **BERT (Bidirectional Encoder Representations from Transformers)** architecture as the foundation for predicting whether a piece of text expresses a **positive, negative, or neutral** sentiment.\n",
    "\n",
    "## 1. What is BERT?\n",
    "- **BERT** is a deep learning model developed by Google (2018).\n",
    "- Built on the **transformer** architecture and is **bidirectional**, meaning it looks at the entire sentence both left-to-right and right-to-left when learning context.\n",
    "- Pretrained on large text corpora (Wikipedia, BooksCorpus), so it already understands a lot of language before fine-tuning.\n",
    "\n",
    "## 2. How it’s Used in Sentiment Analysis\n",
    "- **Fine-tuning**: Start with pretrained BERT and train it further on a labeled sentiment dataset (e.g., movie reviews with “positive/negative” labels).\n",
    "- The final classification layer is usually a **softmax output** that predicts sentiment categories.\n",
    "- Example:  \n",
    "  Input = *“This movie was amazing!”* → Output = *Positive (0.95 probability)*.\n",
    "\n",
    "## 3. Advantages of BERT in Sentiment Analysis\n",
    "- **Contextual understanding**: Captures meaning based on surrounding words (e.g., “bank” in *river bank* vs *investment bank*).\n",
    "- **Handles subtle sentiment**: Detects sarcasm, negation (“not bad” = positive), or mixed tones.\n",
    "- **Transfer learning**: Pretraining gives it strong language understanding, so it performs well with relatively small sentiment datasets.\n",
    "\n",
    "## 4. Variants\n",
    "- **DistilBERT, RoBERTa, ALBERT** – lighter or optimized BERT-based models.\n",
    "- **Domain-specific BERT** – e.g., *FinBERT* (finance), *BioBERT* (biomedical), *BERTweet* (tweets) for specialized sentiment tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d568a",
   "metadata": {},
   "source": [
    "# Apply BERT based model for Sentiment Analysis\n",
    "## Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc230a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# create a function to generate confusion matrix and classification report for sentiment analysis\n",
    "\n",
    "def classification_evaluation(y_true, y_pred):\n",
    "    classes = [\"Positive\", \"Neutral\", \"Negative\"]\n",
    "\n",
    "    # generate Confusion matrix (fixed label order)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(\"Confusion Matrix - Sentiment Analysis\")\n",
    "    plt.show()\n",
    "\n",
    "    # generate classification report with the SAME labels & names. Use three decimal places\n",
    "    print(classification_report(y_true, y_pred, labels=classes, target_names=classes, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc1539-d928-4612-802c-cd78340b6710",
   "metadata": {},
   "source": [
    "## Load default sentiment-analysis pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d0b7b-c8bb-4279-91c3-74c67caf0b14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we will use default model: distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db7d0e-c35b-4915-ba20-d862be3c22f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run sentiment analysis based on a single text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c6c938-2fec-465f-9e04-75b363d2c453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review1=\"Amazing stay! The apartment was spotless and the host was super helpful.\"\n",
    "\n",
    "sentiment_pipeline(review1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed70d0-e4f7-43b8-b240-75e792daf55c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review2=\"We do not have a good experience. The room is too old and there is loud noise during the night\"\n",
    "\n",
    "sentiment_pipeline(review2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161fbbed-7b11-41d1-85f6-4c606597de8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review3=\"The location is great, but the bed was uncomfortable.\" \n",
    "sentiment_pipeline(review3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7f52d-4f9d-4820-a4a1-341e21f6f725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review4_in_Spanish=\"El departamento estÃ¡ muy bien ubicado en una muy buena zona cerca de Boston Common.\\\n",
    "EstÃ¡ muy bien equipado con todo lo necesario y de muy buen gusto. Michael nos recibiÃ³ muy amablemente brindando todas las indicaciones \\\n",
    "sobre el departamento e informaciÃ³n sobre el barrio y sus comercios. Merece destacar que nos obsequiÃ³ con un excelente vino que disfrutamos\\\n",
    "en la hermosa terraza.\"\n",
    "\n",
    "review4_in_English=\"The apartment is very well located in a great area near Boston Common. \\\n",
    "It is very well equipped with everything necessary and decorated with very good taste. \\\n",
    "Michael welcomed us very kindly, providing all the instructions about the apartment and information \\\n",
    "about the neighborhood and its shops. It is worth mentioning that he gifted us an excellent bottle of wine, which we enjoyed on the beautiful terrace.\"\n",
    "\n",
    "result1=sentiment_pipeline(review4_in_Spanish)\n",
    "\n",
    "result2=sentiment_pipeline(review4_in_English)\n",
    "\n",
    "print(f'\"Review in Spanish: \"{result1}\\n\"Review in English: \" {result2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5adc9c7-7351-4cf3-bc84-ca3d32dda4c4",
   "metadata": {},
   "source": [
    " **Note:** The base BERT model was trained in English; therefore, reviews written in other languages may yield unpredictable results. This limitation is common on social media platforms and e-commerce sites with global users or U.S.-based users who choose to write in another language. Based on my experience, BERT often predicts such non-English reviews as *Neutral*, since it cannot reliably detect emotions expressed outside of English.  \n",
    ">\n",
    "> 💡 **Tip:** For datasets that include multiple languages, consider using **Multilingual BERT (mBERT)** or **XLM-RoBERTa**, which are trained on many languages and better suited for handling multilingual text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8b8ed-2817-44f3-b080-405c73fdc6c9",
   "metadata": {},
   "source": [
    "## Load a different model\n",
    "- The text classification task in this model is based on 3 sentiment labels.\n",
    "- You can check more models from [here](https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb90ed-4d23-45c8-901c-d3c300010d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "inputs = [review1, review2, review3, review4_in_Spanish, review4_in_English ]\n",
    "\n",
    "# set retunr_all_scores to true to return the score for all lables. Otherwise, it will only return the label with the highest probability.\n",
    "\n",
    "result = sentiment_pipeline(inputs, return_all_scores=True)\n",
    "\n",
    "label_mapping = {\"LABEL_0\": \"Negative\", \"LABEL_1\": \"Neutral\", \"LABEL_2\": \"Positive\"}\n",
    "\n",
    "for i, predictions in enumerate(result):\n",
    "  print(\"==================================\")\n",
    "  print(f\"Text {i + 1}: {inputs[i]}\")\n",
    "  for pred in predictions:\n",
    "    label = label_mapping.get(pred['label'], pred['label'])\n",
    "    score = pred['score']\n",
    "    print(f\"{label}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd5b28",
   "metadata": {},
   "source": [
    "## Apply the twitter-roberta model to Boston airbnb review data\n",
    "The Airbnb review dataset does not contain predefined sentiment labels. To evaluate the model’s performance, I manually read the reviews and assigned sentiment categories (Positive, Neutral, Negative). The model’s predictions were then compared against these human-generated labels to assess accuracy.\n",
    "\n",
    "For illustration, I labeled only 20 reviews.\n",
    "\n",
    "In practice, manually labeling every review would be impractical. A common approach is to use the review rating (the star score people provided when submitting a review) as a proxy for sentiment. In addition, we can leverage large language models (e.g., ChatGPT) to generate sentiment labels automatically. We will explore this approach in later weeks when discussing generative AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d878021-4da5-4344-a826-678d4ed8ac0a",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0e2ea-de88-437f-9f8a-667721265edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('data/boston_airbnb_reviews_with_human_labels.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59915a30-b358-40a3-b868-94bf60f95924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e319fc-8036-4efa-a5b1-543ab95a0181",
   "metadata": {},
   "source": [
    "### Apply the BERT model to comments column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a948d9-3dac-4456-9d70-c1a6a76c9362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "#sentiment_pipeline = pipeline(model=\"delarosajav95/tw-roberta-base-sentiment-FT-v2\")\n",
    "\n",
    "label_mapping = {\"LABEL_0\": \"Negative\", \"LABEL_1\": \"Neutral\", \"LABEL_2\": \"Positive\"}\n",
    "\n",
    "# covert comments columns to list\n",
    "\n",
    "reviews = df['comments'].tolist()\n",
    "\n",
    "# pipeline returns a list of dicts.\n",
    "# batch_size=32: processes up to 32 reviews at once (faster than one by one).\n",
    "# truncation=True: ensures text longer than the max length is truncated.\n",
    "# max_length=512: the maximum number of tokens the model can process (512 is standard for RoBERTa).\n",
    "\n",
    "outputs = sentiment_pipeline(reviews, batch_size=32, truncation=True, max_length=512)\n",
    "\n",
    "# Map labels and collect scores\n",
    "\n",
    "df['sentiment'] = [label_mapping.get(o['label'], o['label']) for o in outputs]\n",
    "df['sentiment_score'] = [o['score'] for o in outputs]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9682028-32b8-429b-82d9-f89776d2249b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8009bd4-39c7-4e06-87c8-0ba8c800fc96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check number of reviews by human label\n",
    "\n",
    "df['human_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb798914-0b7e-4d78-b2a9-3ca0b0d515f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check number of reviews by predicted sentiment\n",
    "\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30defd-55be-4c8a-8275-5be6fa8a402e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualize the sentiment distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065deec9-c709-418a-b849-5a7fd6d6006b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result=df['sentiment'].value_counts().reset_index()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4462cdaf-b33f-459f-96d3-32f919074796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define custom colors (order must match labels)\n",
    "colors = [\"green\", \"yellow\", \"red\"]\n",
    "\n",
    "# Create pie chart\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(\n",
    "    result['count'],\n",
    "    labels=result['sentiment'],\n",
    "    autopct='%1.1f%%',       # Show percentages\n",
    "    startangle=140,          # Rotate for better appearance\n",
    "    colors=colors,           # Apply custom colors\n",
    "    wedgeprops={'edgecolor': 'white'}\n",
    ")\n",
    "\n",
    "plt.title(\"Sentiment Distribution in Boston Airbnb Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1cbd5-cbda-45f8-abc2-a67c6b7653ec",
   "metadata": {},
   "source": [
    "### Look at negative comments predicted by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db674f-7479-4164-a025-c51cb204f86e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change column width to see full comments.\n",
    "pd.set_option(\"display.max_colwidth\", None) \n",
    "\n",
    "df[df['sentiment']=='Negative']['comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820506dc-91aa-42a2-85b9-de50b2d423d1",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance\n",
    "\n",
    "Compare the human generated label with the prediction from BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e7f5d-98d1-4825-8cb4-94f3ecef0359",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model performance\n",
    "\n",
    "y_true = df['human_label']\n",
    "y_pred = df['sentiment']\n",
    "    \n",
    "classification_evaluation(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106281c4-be7f-4101-aa27-5d288e35d1b7",
   "metadata": {},
   "source": [
    "## Sentiment Classification Results Interpretation\n",
    "\n",
    "## Key Metrics and Formulas\n",
    "\n",
    "1. **Accuracy**  \n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Predictions}}\n",
    "$$\n",
    "\n",
    "2. **Precision (per class)**  \n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n",
    "$$  \n",
    "Measures how many predicted positives are actually correct.\n",
    "\n",
    "3. **Recall (per class)**  \n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
    "$$  \n",
    "Measures how many actual positives were correctly identified.\n",
    "\n",
    "4. **F1 Score (per class)**  \n",
    "$$\n",
    "\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n",
    "$$  \n",
    "Balances precision and recall into a single metric.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation of Results\n",
    "\n",
    "**1. Confusion Matrix**  \n",
    "- **Positive reviews**: Out of 16 true positives, the model correctly classified all 16 (perfect accuracy for this class).  \n",
    "- **Neutral reviews**: Only 1 review labeled as Neutral, and the model correctly predicted it.  \n",
    "- **Negative reviews**: Out of 3 true negatives, the model correctly identified 1 but misclassified 2 as Neutral.  \n",
    "\n",
    "**2. Metrics Breakdown**  \n",
    "- **Positive Class**:  \n",
    "  - Precision = 1.00, Recall = 1.00, F1 = 1.00  \n",
    "  - The model performs perfectly here, likely because this class dominates the dataset (16/20).  \n",
    "\n",
    "- **Neutral Class**:  \n",
    "  - Precision = 0.33, Recall = 1.00, F1 = 0.50  \n",
    "  - The model captured the single Neutral review but also misclassified 2 Negatives as Neutral, leading to low precision.  \n",
    "\n",
    "- **Negative Class**:  \n",
    "  - Precision = 1.00, Recall = 0.33, F1 = 0.50  \n",
    "  - The model is precise when it predicts Negative, but it misses most true Negative cases (low recall).  \n",
    "\n",
    "**3. Overall Performance**  \n",
    "- **Accuracy** = 90% (18 out of 20 reviews correctly classified).  \n",
    "- **Macro Average (equal weight per class)**: F1 = 0.667 → reflects imbalance between classes.  \n",
    "- **Weighted Average (weighted by support)**: F1 = 0.900 → high overall because Positive dominates.  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- The model is **very strong at detecting Positive sentiment** (likely due to class imbalance).  \n",
    "- Performance drops for **Neutral and Negative** classes, especially recall for Negative (only 1 of 3 detected).  \n",
    "- With such a small sample (20 reviews, heavily skewed toward Positive), these results should be interpreted cautiously.  \n",
    "- A larger, more balanced labeled dataset is needed for a fair evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI500Env",
   "language": "python",
   "name": "ai500env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
